â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          LAG-LLAMA INTEGRATION - READY TO USE!                        â•‘
â•‘     Time-Series Foundation Model for IoT Forecasting                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… SETUP COMPLETE!

Your application is now configured to use Lag-Llama, the first open-source
foundation model specifically designed for time-series forecasting.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸš€ QUICK START (Choose One)

Option 1: Easy Start (One Command)
   ./start-lag-llama.sh

Option 2: Manual Start
   Terminal 1:
   cd lag-llama-server
   ./setup.sh
   source venv/bin/activate
   python server.py

   Terminal 2:
   npm run dev

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ NEW FILES CREATED

â”œâ”€â”€ lag-llama-server/              # Inference server
â”‚   â”œâ”€â”€ server.py                  # Flask API server
â”‚   â”œâ”€â”€ requirements.txt           # Python dependencies
â”‚   â”œâ”€â”€ setup.sh                   # Setup script
â”‚   â””â”€â”€ README.md                  # Server docs
â”œâ”€â”€ LAG_LLAMA_SETUP.md            # Complete setup guide
â”œâ”€â”€ start-lag-llama.sh            # Quick start script
â””â”€â”€ .env.local                     # âœ… Pre-configured!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ CURRENT CONFIGURATION

Your .env.local is configured:
   LLM_BASE_URL=http://localhost:8000/v1/chat/completions
   LLM_API_KEY=not-needed-for-local
   LLM_MODEL_NAME=lag-llama

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ WHAT IS LAG-LLAMA?

Lag-Llama is a foundation model for time-series forecasting:

âœ… Purpose-Built         Designed specifically for time-series
âœ… Zero-Shot            Works on any frequency, any horizon
âœ… Open-Source          Free to use, no API costs
âœ… Probabilistic        Provides P10/P50/P90 predictions
âœ… Research-Backed      Published model with strong results
âœ… Fine-Tunable         Can be trained on your data

Paper: https://arxiv.org/abs/2310.08278
Repo:  https://github.com/time-series-foundation-models/lag-llama

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ§ª TESTING

1. Start Lag-Llama server:
   ./start-lag-llama.sh

2. Test server health:
   curl http://localhost:8000/health

3. Start Next.js app (in another terminal):
   npm run dev

4. Visit http://localhost:3000

5. Upload sample CSV and generate forecast!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š TWO MODES

Development Mode (Current):
   â€¢ Quick to set up
   â€¢ Simplified forecasting algorithm
   â€¢ Good for testing integration
   â€¢ No model download needed

Production Mode:
   â€¢ Download actual Lag-Llama model (~300MB)
   â€¢ State-of-the-art forecasting
   â€¢ GPU acceleration support

   To use production mode:
   cd lag-llama-server
   source venv/bin/activate
   huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir .

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ SWITCHING BETWEEN MODELS

Lag-Llama (Local) - Current:
   LLM_BASE_URL=http://localhost:8000/v1/chat/completions
   LLM_API_KEY=not-needed

OpenAI:
   LLM_BASE_URL=https://api.openai.com/v1/chat/completions
   LLM_API_KEY=your-api-key-here

Edit .env.local to switch!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“š DOCUMENTATION

â€¢ LAG_LLAMA_SETUP.md    - Complete setup guide
â€¢ lag-llama-server/README.md - Server documentation
â€¢ README.md             - Main app documentation

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ‰ YOU'RE ALL SET!

Your IoT Forecaster is now powered by Lag-Llama, a state-of-the-art
time-series foundation model.

Run: ./start-lag-llama.sh to begin!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
